{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiamo le librerie\n",
    "\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabili globali\n",
    "\n",
    "mypath = 'dataset'\n",
    "\n",
    "# Mi vado a prendere i path di tutti i file nella cartella dataset\n",
    "\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "# Mi vado a generare tutte le possibili combinazioni di due elementi da un insieme di 5 elementi\n",
    "\n",
    "\n",
    "comb = list(combinations(range(0,5), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andiamo a leggere i file pickle per importare i dataset\n",
    "\n",
    "all = []\n",
    "\n",
    "all_document = []\n",
    "\n",
    "document = 'Friday-02-03-2018_TrafficForML_CICFlowMeter'\n",
    "\n",
    "for a in onlyfiles:\n",
    "    b = a.replace('.csv','')\n",
    "    with open('pickle/'+ document +'/scaled_document.pickle', 'rb') as handle:\n",
    "        all.append(pickle.load(handle))\n",
    "\n",
    "    with open('pickle/'+'Friday-02-03-2018_TrafficForML_CICFlowMeter'+'/scaled_document_plot.pickle', 'rb') as handle:\n",
    "        all_document.append(pickle.load(handle))\n",
    "\n",
    "for i, j in enumerate(onlyfiles):\n",
    "    if j == document:\n",
    "        idx = i\n",
    "\n",
    "folder = onlyfiles[idx]\n",
    "folder = folder.replace('.csv','')\n",
    "print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andiamo a pulire tutti i dataset\n",
    "\n",
    "# Come prima cosa andiamo a leggere le feature dai vari file csv\n",
    "\n",
    "'''for a in onlyfiles:\n",
    "    '''\n",
    "anova = pd.read_csv('top_feature/'+'Friday-23-02-2018_TrafficForML_CICFlowMeter'+\"/anova.csv\")\n",
    "tree = pd.read_csv('top_feature/'+'Friday-23-02-2018_TrafficForML_CICFlowMeter'+\"/ExtraTree.csv\")\n",
    "svm = pd.read_csv('top_feature/'+'Friday-23-02-2018_TrafficForML_CICFlowMeter'+\"/SVM.csv\")\n",
    "rfe = pd.read_csv('top_feature/'+'Friday-23-02-2018_TrafficForML_CICFlowMeter'+\"/RFE.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facciamo una concatenazione dei vari dataframe.\n",
    "# Saranno utilizzati per prendere le feature che si ripetono di più e che hanno i punteggi maggiori\n",
    "\n",
    "total_feature = pd.concat([anova,tree,svm,rfe],ignore_index=True)\n",
    "total_feature_count = pd.concat([anova,tree,svm,rfe],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andiamo a eseguire un operazione di group by, con successivo count delle occorrenze dei vari campi\n",
    "\n",
    "total_feature_count = total_feature_count.groupby(['Input_Features']).count().reset_index().sort_values(by='F_Score',ascending=False)\n",
    "\n",
    "# Andiamo a rinominare la colonna F_Score, che adesso contiene i valori di count\n",
    "\n",
    "total_feature_count.rename(columns = {'F_Score':'Conto'}, inplace = True)\n",
    "print(total_feature_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delle delle feature più frequenti mi interessano le prime dieci\n",
    "\n",
    "most_frequent = total_feature_count[:10]\n",
    "\n",
    "print(most_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ci serve ancora il parametro F_Scores, quindi creiamo un nuovo dataframe con queste righe.\n",
    "# Abbiamo le dieci righe, però, sono ancora presenti le ridondanze che prima avevamo eliminato.\n",
    "\n",
    "test = pd.DataFrame(data=[],columns=total_feature.columns)\n",
    "\n",
    "for a in most_frequent['Input_Features']:\n",
    "    test = pd.concat([test,total_feature[total_feature['Input_Features'] == a]])\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo una variabile all'interno del quale vado a salvare il groupby del precedente dataframe, solo che in uesto caso\n",
    "# andiamo a sommare i punteggi di F_Scores, invece di contare le occorrenze.\n",
    "\n",
    "group_by = test.groupby(['Input_Features'])['F_Score'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(group_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facciamo un ordinamento del dataframe precedente\n",
    "\n",
    "print(group_by.sort_values(by='F_Score',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A questo punto, terminiamo l'operazione andando a moltiplicare la frequenza per la somma dei punteggi delle varie feature.\n",
    "\n",
    "last_feature = pd.DataFrame(data=[],columns=group_by.columns)\n",
    "\n",
    "for a,b in zip(most_frequent['Input_Features'],most_frequent['Conto']):\n",
    "    last_feature.loc[len(last_feature)] =  {'Input_Features':a,'F_Score':group_by[group_by['Input_Features'] == a]['F_Score'].values[0]*b}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facciamo una stampa ordinata\n",
    "\n",
    "print(last_feature.sort_values(by='F_Score',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A questo punto ci interessano solo le prime 5 feature, e tutte le loro possibili combinazioni di due elementi.\n",
    "\n",
    "esito_finale = pd.DataFrame(data=[],columns=['Feature_1','Feature_2'])\n",
    "top_index = []\n",
    "\n",
    "# Qui andiamo a prendere gli indici delle prime 5 features\n",
    "\n",
    "for a in range(0,5):\n",
    "    top_index.append(last_feature.sort_values(by='F_Score',ascending=False).index[a])\n",
    "    \n",
    "# Andiamo a inserire tutte le possibili combinazioni nel dataframe\n",
    "\n",
    "for b in comb:\n",
    "    print(last_feature.loc[top_index[b[0]]]['Input_Features'])\n",
    "    print(last_feature.loc[top_index[b[1]]]['Input_Features'])\n",
    "    esito_finale.loc[len(esito_finale)] =  {\n",
    "        'Feature_1': last_feature.loc[top_index[b[0]]]['Input_Features'],\n",
    "        'Feature_2': last_feature.loc[top_index[b[1]]]['Input_Features']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(esito_finale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salviamo l'esito finale in un file csv\n",
    "\n",
    "esito_finale.to_csv('top_feature/'+folder+'/combinazioni.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
