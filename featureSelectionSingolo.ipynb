{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In questo documento andiamo a selezionare le feature singolarmente da ogni documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiamo le librerie\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabili globali\n",
    "\n",
    "mypath = 'dataset'\n",
    "\n",
    "# Indice per il file corrente da analizzare \n",
    "\n",
    "current = 9\n",
    "\n",
    "# Questi campi non sono di interesse, quindi li escludiamo dalle analisi\n",
    "\n",
    "first = []\n",
    "\n",
    "if current == 3:\n",
    "    first = ['Dst Port', 'Protocol', 'Timestamp','Flow ID','Src IP','Dst IP']\n",
    "else:\n",
    "    first = ['Dst Port', 'Protocol', 'Timestamp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mi vado a prendere i path di tutti i file nella cartella dataset\n",
    "\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mi creo una cartella image all'interno del quale per ogni file vado a inserire le immagini d'interesse\n",
    "\n",
    "if not os.path.exists(\"image\"):\n",
    "    os.makedirs(\"image\")\n",
    "    for a in onlyfiles:\n",
    "        b = a.replace('.csv','')\n",
    "        os.makedirs(\"image/\"+b)\n",
    "        os.makedirs(\"image/\"+b+\"/scatter\")\n",
    "        os.makedirs(\"image/\"+b+\"/istogrammi\")\n",
    "\n",
    "# Se non esiste, creo una cartella dove salvare le migliori feature\n",
    "\n",
    "if not os.path.exists(\"top_feature\"):\n",
    "    os.makedirs(\"top_feature\")\n",
    "    for a in onlyfiles:\n",
    "        b = a.replace('.csv','')\n",
    "        os.makedirs(\"top_feature/\"+b)\n",
    "\n",
    "if not os.path.exists(\"pickle\"):\n",
    "    os.makedirs(\"pickle\")\n",
    "    for a in onlyfiles:\n",
    "        b = a.replace('.csv','')\n",
    "        os.makedirs(\"pickle/\"+b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per ogni documento andiamo a eseguire le operazioni di pre-processing, e a selezionare le feature d'interesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_15328\\3663514834.py:1: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  document= pd.read_csv(mypath+'/'+onlyfiles[current])\n"
     ]
    }
   ],
   "source": [
    "document= pd.read_csv(mypath+'/'+onlyfiles[current])\n",
    "scarto = []\n",
    "scarto.append([])\n",
    "folder = onlyfiles[current].replace('.csv','')\n",
    "attaccanti_label = document['Label'].unique()[document['Label'].unique() != 'Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andiamo a eliminare i primi 3 campi inutili\n",
    "\n",
    "for a in first:\n",
    "    document.drop(columns=a,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisi della varianza, eliminiamo tutte le feature che hanno un solo valore\n",
    "\n",
    "for a in document.head(0):\n",
    "    \n",
    "    if(document[a].unique().shape[0] == 1):\n",
    "        scarto[-1].append(a)\n",
    "        document.drop(columns=a,axis=1,inplace=True)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo un duplicato solo per il plot \n",
    "\n",
    "document_plot = document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andiamo a eliminare tutti i campi dove sono presenti delle stringhe come valori\n",
    "for label in document.head(0):\n",
    "    document = document[document[label] != label]\n",
    "    document_plot = document_plot[document_plot[label] != label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andiamo a sostituire le label con valori interi\n",
    "\n",
    "for b,c in zip(attaccanti_label,range(len(attaccanti_label))):\n",
    "    document = document.replace(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nel documento sono presenti valori infiniti, li andiamo a sostituire con Nan che verranno successivamente rimossi\n",
    "\n",
    "document.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "document_plot.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminiamo i valori nulli\n",
    "\n",
    "document.dropna(inplace=True)\n",
    "document_plot.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andiamo a castare tutte le stringhe rimanenti in numeri, in quanto alcuni numeri sono rappresentati come stringhe\n",
    "\n",
    "document = document.astype(float)\n",
    "for label in document_plot.head(0):\n",
    "    if(label != 'Label'):\n",
    "        try:\n",
    "            document_plot[label] = document_plot[label].astype(float)\n",
    "        except:\n",
    "            print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'indici = []\\n\\nfor a in document.head(0):\\n    for valore,indice in zip(document[a],document[a].index):\\n        if type(valore) == str:\\n            indici.append(indice)\\n\\n\\nif(len(indici) != 0):\\n    \\n    document.drop(indici,inplace=True,axis=0)\\n    document_plot.drop(indici,inplace=True,axis=0)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dove presenti vanno eliminati i valori str dalle righe\n",
    "\n",
    "'''indici = []\n",
    "\n",
    "for a in document.head(0):\n",
    "    for valore,indice in zip(document[a],document[a].index):\n",
    "        if type(valore) == str:\n",
    "            indici.append(indice)\n",
    "\n",
    "\n",
    "if(len(indici) != 0):\n",
    "    \n",
    "    document.drop(indici,inplace=True,axis=0)\n",
    "    document_plot.drop(indici,inplace=True,axis=0)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0.0\n",
      "1         0.0\n",
      "2         0.0\n",
      "3         0.0\n",
      "4         0.0\n",
      "         ... \n",
      "613099    1.0\n",
      "613100    1.0\n",
      "613101    0.0\n",
      "613102    1.0\n",
      "613103    1.0\n",
      "Name: Label, Length: 607690, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(document.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisco una funzione per normalizzare i valori\n",
    "\n",
    "def normalize(feature:str):\n",
    "    return (document[feature]-document[feature].mean())/np.std(document[feature])\n",
    "\n",
    "def normalize_plot(feature:str):\n",
    "    return (document_plot[feature]-document_plot[feature].mean())/np.std(document_plot[feature])\n",
    "\n",
    "\n",
    "# Andiamo a normalizzare i valori per poter usare i vari algoritmi di feature selection\n",
    "\n",
    "scaled_document = document.head(0)\n",
    "scaled_document_plot = document_plot.head(0)\n",
    "\n",
    "for a in document.head(0):\n",
    "    \n",
    "    try:\n",
    "        if document[a].max() != 0 and a != 'Label':\n",
    "            scaled_document[a] = normalize(a)\n",
    "            scaled_document_plot[a] = normalize_plot(a)\n",
    "        else:\n",
    "            scaled_document[a] = document[a]\n",
    "            scaled_document_plot[a] = document_plot[a]\n",
    "    except:\n",
    "\n",
    "        print(a)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A questo punto andiamo a salvare gli histogrammi delle varie feature per effettuare analisi visive\n",
    "\n",
    "for h in scaled_document.head(0):\n",
    "\n",
    "    # Stampo un istogramma per ogni valori di label, cos√¨ da avere la suddivisione per colore\n",
    "    \n",
    "    for b,c in zip(attaccanti_label,range(len(attaccanti_label))):\n",
    "        scaled_document[scaled_document['Label'] == c][h].hist(label=str(b))\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.title(h)\n",
    "    feature_name = h.replace('/','_')\n",
    "    plt.savefig('image/'+folder+'/'+'istogrammi/'+str(feature_name)+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vado a suddividere il dataset in x e y per applicare successivamente, i vari algoritmi di feature selection\n",
    "\n",
    "x_selection = scaled_document.iloc[:,:-1]\n",
    "y_selection = scaled_document.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(607690, 1)\n",
      "1215380\n"
     ]
    }
   ],
   "source": [
    "print(y_selection.shape)\n",
    "print(x_selection.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSelectKBest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Applico il test Anova sul dataset\u001b[39;00m\n\u001b[0;32m      3\u001b[0m fvalue_Best \u001b[39m=\u001b[39m SelectKBest(score_func\u001b[39m=\u001b[39mf_classif, k\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m fit \u001b[39m=\u001b[39m fvalue_Best\u001b[39m.\u001b[39;49mfit(x_selection, y_selection\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mint\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\envs\\tensor\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:467\u001b[0m, in \u001b[0;36m_BaseFilter.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run score function on (X, y) and get the appropriate features.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \n\u001b[0;32m    451\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 467\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    468\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m], multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    469\u001b[0m )\n\u001b[0;32m    471\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params(X, y)\n\u001b[0;32m    472\u001b[0m score_func_ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore_func(X, y)\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\envs\\tensor\\lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\envs\\tensor\\lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\envs\\tensor\\lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\pc\\anaconda3\\envs\\tensor\\lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nSelectKBest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "\n",
    "# Applico il test Anova sul dataset\n",
    "\n",
    "fvalue_Best = SelectKBest(score_func=f_classif, k=10)\n",
    "fit = fvalue_Best.fit(x_selection, y_selection.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andiamo a stampare le migliori dieci feature selezionate da Anova su un file csv\n",
    "\n",
    "features_score = pd.DataFrame(fit.scores_)\n",
    "features = pd.DataFrame(x_selection.columns)\n",
    "feature_score = pd.concat([features,features_score],axis=1)\n",
    "\n",
    "# Assegniamo un nuovo nome alle colonne\n",
    "\n",
    "feature_score.columns = [\"Input_Features\",\"F_Score\"]\n",
    "print(feature_score.nlargest(10,columns=\"F_Score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mi vado a salvare le migliori 10 feature ottenute dal test Anova con il relativo punteggio\n",
    "\n",
    "feature_score.nlargest(10,columns=\"F_Score\").to_csv('top_feature/'+folder+'/anova.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_selection.head(0),y_selection.head(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizziamo altri metodi di feature extraction per prendere le migliori 10 feature\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(x_selection, y_selection.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_score = pd.DataFrame(clf.feature_importances_)\n",
    "features = pd.DataFrame(x_selection.columns)\n",
    "feature_score = pd.concat([features,features_score],axis=1)\n",
    "\n",
    "# Assegniamo un nuovo nome alle colonne\n",
    "\n",
    "feature_score.columns = [\"Input_Features\",\"F_Score\"]\n",
    "print(feature_score.nlargest(15,columns=\"F_Score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mi vado a salvare le migliori 10 feature ottenute dal test Anova con il relativo punteggio\n",
    "\n",
    "feature_score.nlargest(10,columns=\"F_Score\").to_csv('top_feature/'+folder+'/ExtraTree.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizziamo SVM come metodo di feature selection\n",
    "\n",
    "features_names = x_selection.head(0)\n",
    "svm = svm.SVC(kernel='linear',max_iter=400)\n",
    "svm.fit(x_selection, y_selection.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questa √® una funziona che mi permette di visualizzare le migliori feature\n",
    "\n",
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names[-10:])), imp[-10:], align='center')\n",
    "    plt.yticks(range(len(names[-10:])), names[-10:])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "f_importances(abs(svm.coef_[0]), features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_svm = pd.DataFrame()\n",
    "\n",
    "for a in sorted(zip(abs(svm.coef_[0]),features_names),reverse=True):\n",
    "    top_svm = pd.concat([top_svm, pd.DataFrame([a])], ignore_index=True)\n",
    "\n",
    "top_svm.rename(columns = {0:'F_Score', 1:'Input_Features'}, inplace = True)\n",
    "print(top_svm.head(10))\n",
    "\n",
    "top_svm.nlargest(10,columns=\"F_Score\").to_csv('top_feature/'+folder+'/SVM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultimo metodo di feature selection che utilizziamo √® quello RFE, che sta per Recorsive Feature Elimination\n",
    "\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=10)\n",
    "# fit RFE\n",
    "rfe.fit(x_selection, y_selection.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andiamo a salvare le feature in un file csv\n",
    "\n",
    "features_score = pd.DataFrame(rfe.ranking_)\n",
    "features = pd.DataFrame(x_selection.columns)\n",
    "feature_score = pd.concat([features,features_score],axis=1)\n",
    "\n",
    "feature_score.columns = [\"Input_Features\",\"F_Score\"]\n",
    "feature_score.sort_values('F_Score',inplace=True,ascending=False,axis=0)\n",
    "feature_score.nsmallest(10,columns=\"F_Score\").to_csv('top_feature/'+folder+'/RFE.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per non creare un solo file troppo ricco di codice salviamo i dataset in formato pickle \n",
    "# per utilizzarlo successivamente in un altro file\n",
    "\n",
    "with open('pickle/'+folder+'/scaled_document.pickle', 'wb') as handle:\n",
    "    pickle.dump(scaled_document, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    handle.close()\n",
    "with open('pickle/'+folder+'/scaled_document_plot.pickle', 'wb') as handle:\n",
    "    pickle.dump(scaled_document_plot, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    handle.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
