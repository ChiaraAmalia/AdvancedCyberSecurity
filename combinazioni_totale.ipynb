{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In questo file andiamo ad individuare le 5 feature più importanti per fare clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiamo le librerie\n",
    "\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\ProgettoAdvanced\\AdvancedCyberSecurity\n"
     ]
    }
   ],
   "source": [
    "path_file = os.path.abspath(os.getcwd()) #prendiamo il path in cui si trova il file su cui stiamo lavorando\n",
    "print(path_file)\n",
    "os.chdir(path_file) #cambiamo directory al fine di poter prendere i file csv per l'analisi della selezione delle features\n",
    "data = r\"\\top_feature\"\n",
    "\n",
    "comb = list(combinations(range(0,5), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\ProgettoAdvanced\\AdvancedCyberSecurity\\top_feature\n"
     ]
    }
   ],
   "source": [
    "final_path = path_file + data\n",
    "print(final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova = pd.DataFrame() #creazione di un dataframe vuoto\n",
    "extra_tree = pd.DataFrame() #creazione di un dataframe vuoto\n",
    "rfe = pd.DataFrame() #creazione di un dataframe vuoto\n",
    "svm = pd.DataFrame() #creazione di un dataframe vuoto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for cart_corrente, sottocartelle, files in os.walk(final_path):\n",
    "    for cartella in sottocartelle:\n",
    "        results = os.path.join(final_path,cartella)\n",
    "        for cartella_corrente, sottocart, file in os.walk(results):\n",
    "            for csv_file in file:\n",
    "                file_corrente = os.path.join(results,csv_file)\n",
    "                df_csv = pd.read_csv(file_corrente)\n",
    "                if file_corrente.__contains__(\"anova.csv\"):\n",
    "                    anova = pd.concat([anova,df_csv],ignore_index=True)\n",
    "                elif file_corrente.__contains__(\"ExtraTree.csv\"):\n",
    "                    extra_tree = pd.concat([extra_tree,df_csv],ignore_index=True)\n",
    "                elif file_corrente.__contains__(\"RFE.csv\"):\n",
    "                    rfe = pd.concat([rfe,df_csv],ignore_index=True)\n",
    "                elif file_corrente.__contains__(\"SVM.csv\"):\n",
    "                    svm = pd.concat([svm,df_csv],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andiamo a normalizzare tutti gli score delle feature, in maniera tale da avere valori compresi tra 0 e 1\n",
    "\n",
    "anova['F_Score'] = anova['F_Score']/anova['F_Score'].max()\n",
    "extra_tree['F_Score'] = extra_tree['F_Score']/extra_tree['F_Score'].max()\n",
    "svm['F_Score'] = svm['F_Score']/svm['F_Score'].max()\n",
    "rfe['F_Score'] = rfe['F_Score']/rfe['F_Score'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facciamo una concatenazione dei vari dataframe.\n",
    "# Saranno utilizzati per prendere le feature che si ripetono di più e che hanno i punteggi maggiori\n",
    "\n",
    "total_feature = pd.concat([anova,extra_tree,svm,rfe],ignore_index=True)\n",
    "total_feature_count = pd.concat([anova,extra_tree,svm,rfe],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Input_Features  Conto\n",
      "16        Flow Pkts/s      6\n",
      "25         Fwd Pkts/s      5\n",
      "29  Init Bwd Win Byts      5\n",
      "13      Flow IAT Mean      4\n",
      "30  Init Fwd Win Byts      4\n",
      "2     Bwd Pkt Len Max      3\n",
      "3    Bwd Pkt Len Mean      3\n",
      "8        ECE Flag Cnt      3\n",
      "19       Fwd IAT Mean      3\n",
      "32        Pkt Len Max      3\n",
      "15       Flow IAT Std      2\n",
      "24   Fwd Pkt Len Mean      2\n",
      "21        Fwd IAT Std      2\n",
      "1         Bwd IAT Tot      2\n",
      "0        ACK Flag Cnt      2\n",
      "14       Flow IAT Min      2\n",
      "27   Fwd Seg Size Min      2\n",
      "12       Flow IAT Max      2\n",
      "11      Flow Duration      2\n",
      "10        Flow Byts/s      2\n",
      "34       Pkt Size Avg      2\n",
      "6    Bwd Seg Size Avg      2\n",
      "5          Bwd Pkts/s      2\n",
      "35       RST Flag Cnt      2\n",
      "31       PSH Flag Cnt      1\n",
      "33       Pkt Len Mean      1\n",
      "28      Fwd URG Flags      1\n",
      "18        Fwd IAT Max      1\n",
      "26   Fwd Seg Size Avg      1\n",
      "23    Fwd Pkt Len Max      1\n",
      "22        Fwd IAT Tot      1\n",
      "20        Fwd IAT Min      1\n",
      "17     Fwd Header Len      1\n",
      "9        FIN Flag Cnt      1\n",
      "7      CWE Flag Count      1\n",
      "4     Bwd Pkt Len Std      1\n",
      "36       URG Flag Cnt      1\n"
     ]
    }
   ],
   "source": [
    "# Andiamo a eseguire un operazione di group by, con successivo count delle occorrenze dei vari campi\n",
    "\n",
    "total_feature_count = total_feature_count.groupby(['Input_Features']).count().reset_index().sort_values(by='F_Score',ascending=False)\n",
    "\n",
    "# Andiamo a rinominare la colonna F_Score, che adesso contiene i valori di count\n",
    "\n",
    "total_feature_count.rename(columns = {'F_Score':'Conto'}, inplace = True)\n",
    "print(total_feature_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Input_Features  Conto\n",
      "16        Flow Pkts/s      6\n",
      "25         Fwd Pkts/s      5\n",
      "29  Init Bwd Win Byts      5\n",
      "13      Flow IAT Mean      4\n",
      "30  Init Fwd Win Byts      4\n",
      "2     Bwd Pkt Len Max      3\n",
      "3    Bwd Pkt Len Mean      3\n",
      "8        ECE Flag Cnt      3\n",
      "19       Fwd IAT Mean      3\n",
      "32        Pkt Len Max      3\n"
     ]
    }
   ],
   "source": [
    "# Delle delle feature più frequenti mi interessano le prime dieci\n",
    "\n",
    "most_frequent = total_feature_count[:10]\n",
    "\n",
    "print(most_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Input_Features   F_Score\n",
      "11        Flow Pkts/s  0.005158\n",
      "25        Flow Pkts/s  0.417705\n",
      "31        Flow Pkts/s  0.514103\n",
      "44        Flow Pkts/s  0.843071\n",
      "66        Flow Pkts/s  1.000000\n",
      "75        Flow Pkts/s  1.000000\n",
      "13         Fwd Pkts/s  0.001804\n",
      "24         Fwd Pkts/s  0.435594\n",
      "32         Fwd Pkts/s  0.493936\n",
      "45         Fwd Pkts/s  0.827847\n",
      "65         Fwd Pkts/s  1.000000\n",
      "7   Init Bwd Win Byts  0.640212\n",
      "12  Init Bwd Win Byts  0.002768\n",
      "43  Init Bwd Win Byts  0.914249\n",
      "50  Init Bwd Win Byts  0.698144\n",
      "60  Init Bwd Win Byts  1.000000\n",
      "38      Flow IAT Mean  0.370623\n",
      "55      Flow IAT Mean  0.338925\n",
      "64      Flow IAT Mean  1.000000\n",
      "76      Flow IAT Mean  1.000000\n",
      "16  Init Fwd Win Byts  0.001626\n",
      "23  Init Fwd Win Byts  0.486912\n",
      "33  Init Fwd Win Byts  0.468611\n",
      "61  Init Fwd Win Byts  1.000000\n",
      "0     Bwd Pkt Len Max  1.000000\n",
      "27    Bwd Pkt Len Max  0.340459\n",
      "52    Bwd Pkt Len Max  0.448072\n",
      "3    Bwd Pkt Len Mean  0.776309\n",
      "57   Bwd Pkt Len Mean  0.335222\n",
      "67   Bwd Pkt Len Mean  1.000000\n",
      "2        ECE Flag Cnt  0.837328\n",
      "21       ECE Flag Cnt  0.782799\n",
      "48       ECE Flag Cnt  0.744979\n",
      "49       Fwd IAT Mean  0.692241\n",
      "58       Fwd IAT Mean  0.305445\n",
      "74       Fwd IAT Mean  1.000000\n",
      "9         Pkt Len Max  0.561326\n",
      "26        Pkt Len Max  0.367574\n",
      "53        Pkt Len Max  0.447087\n"
     ]
    }
   ],
   "source": [
    "# Ci serve ancora il parametro F_Scores, quindi creiamo un nuovo dataframe con queste righe.\n",
    "# Abbiamo le dieci righe, però, sono ancora presenti le ridondanze che prima avevamo eliminato.\n",
    "\n",
    "test = pd.DataFrame(data=[],columns=total_feature.columns)\n",
    "\n",
    "for a in most_frequent['Input_Features']:\n",
    "    test = pd.concat([test,total_feature[total_feature['Input_Features'] == a]])\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo una variabile all'interno del quale vado a salvare il groupby del precedente dataframe, solo che in uesto caso\n",
    "# andiamo a sommare i punteggi di F_Scores, invece di contare le occorrenze.\n",
    "\n",
    "group_by = test.groupby(['Input_Features'])['F_Score'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Input_Features   F_Score\n",
      "4        Flow Pkts/s  3.780038\n",
      "7  Init Bwd Win Byts  3.255373\n",
      "6         Fwd Pkts/s  2.759181\n",
      "3      Flow IAT Mean  2.709548\n",
      "2       ECE Flag Cnt  2.365106\n",
      "1   Bwd Pkt Len Mean  2.111530\n",
      "5       Fwd IAT Mean  1.997686\n",
      "8  Init Fwd Win Byts  1.957148\n",
      "0    Bwd Pkt Len Max  1.788531\n",
      "9        Pkt Len Max  1.375986\n"
     ]
    }
   ],
   "source": [
    "# Facciamo un ordinamento del dataframe precedente\n",
    "\n",
    "print(group_by.sort_values(by='F_Score',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A questo punto, terminiamo l'operazione andando a moltiplicare la frequenza per la somma dei punteggi delle varie feature.\n",
    "\n",
    "last_feature = pd.DataFrame(data=[],columns=group_by.columns)\n",
    "\n",
    "for a,b in zip(most_frequent['Input_Features'],most_frequent['Conto']):\n",
    "    last_feature.loc[len(last_feature)] =  {'Input_Features':a,'F_Score':group_by[group_by['Input_Features'] == a]['F_Score'].values[0]*b}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Input_Features    F_Score\n",
      "0        Flow Pkts/s  22.680225\n",
      "2  Init Bwd Win Byts  16.276866\n",
      "1         Fwd Pkts/s  13.795903\n",
      "3      Flow IAT Mean  10.838193\n",
      "4  Init Fwd Win Byts   7.828592\n",
      "7       ECE Flag Cnt   7.095318\n",
      "6   Bwd Pkt Len Mean   6.334591\n",
      "8       Fwd IAT Mean   5.993057\n",
      "5    Bwd Pkt Len Max   5.365593\n",
      "9        Pkt Len Max   4.127959\n"
     ]
    }
   ],
   "source": [
    "# Facciamo una stampa ordinata\n",
    "\n",
    "print(last_feature.sort_values(by='F_Score',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A questo punto ci interessano solo le prime 5 feature, e tutte le loro possibili combinazioni di due elementi.\n",
    "\n",
    "esito_finale = pd.DataFrame(data=[],columns=['Feature_1','Feature_2'])\n",
    "top_index = []\n",
    "\n",
    "# Qui andiamo a prendere gli indici delle prime 5 features\n",
    "\n",
    "for a in range(0,5):\n",
    "    top_index.append(last_feature.sort_values(by='F_Score',ascending=False).index[a])\n",
    "    \n",
    "# Andiamo a inserire tutte le possibili combinazioni nel dataframe\n",
    "\n",
    "for b in comb:\n",
    "\n",
    "    esito_finale.loc[len(esito_finale)] =  {\n",
    "        'Feature_1': last_feature.loc[top_index[b[0]]]['Input_Features'],\n",
    "        'Feature_2': last_feature.loc[top_index[b[1]]]['Input_Features']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Feature_1          Feature_2\n",
      "0        Flow Pkts/s  Init Bwd Win Byts\n",
      "1        Flow Pkts/s         Fwd Pkts/s\n",
      "2        Flow Pkts/s      Flow IAT Mean\n",
      "3        Flow Pkts/s  Init Fwd Win Byts\n",
      "4  Init Bwd Win Byts         Fwd Pkts/s\n",
      "5  Init Bwd Win Byts      Flow IAT Mean\n",
      "6  Init Bwd Win Byts  Init Fwd Win Byts\n",
      "7         Fwd Pkts/s      Flow IAT Mean\n",
      "8         Fwd Pkts/s  Init Fwd Win Byts\n",
      "9      Flow IAT Mean  Init Fwd Win Byts\n"
     ]
    }
   ],
   "source": [
    "print(esito_finale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = esito_finale.Feature_1.unique()\n",
    "top5 = np.append(top5,esito_finale.Feature_2.unique())\n",
    "top5 = pd.Series(top5)\n",
    "top5 = top5.unique()\n",
    "top5 = pd.Series(top5)\n",
    "print(top5)\n",
    "\n",
    "top5.to_csv('top_feature/top5_totale.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "esito_finale.to_csv('top_feature/combinazioni_totale.csv',index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
