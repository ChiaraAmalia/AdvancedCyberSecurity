{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In questo file andiamo ad individuare le 5 feature più importanti per fare clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiamo le librerie\n",
    "\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/spalazzi/Downloads/AdvancedCyberSecurity-main\n"
     ]
    }
   ],
   "source": [
    "path_file = os.path.abspath(os.getcwd()) #prendiamo il path in cui si trova il file su cui stiamo lavorando\n",
    "print(path_file)\n",
    "os.chdir(path_file) #cambiamo directory al fine di poter prendere i file csv per l'analisi della selezione delle features\n",
    "data = r\"/top_feature\"\n",
    "\n",
    "comb = list(combinations(range(0,5), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/spalazzi/Downloads/AdvancedCyberSecurity-main/top_feature\n"
     ]
    }
   ],
   "source": [
    "final_path = path_file + data\n",
    "print(final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova = pd.DataFrame() #creazione di un dataframe vuoto\n",
    "extra_tree = pd.DataFrame() #creazione di un dataframe vuoto\n",
    "rfe = pd.DataFrame() #creazione di un dataframe vuoto\n",
    "svm = pd.DataFrame() #creazione di un dataframe vuoto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for cart_corrente, sottocartelle, files in os.walk(final_path):\n",
    "    for cartella in sottocartelle:\n",
    "        results = os.path.join(final_path,cartella)\n",
    "        for cartella_corrente, sottocart, file in os.walk(results):\n",
    "            for csv_file in file:\n",
    "                file_corrente = os.path.join(results,csv_file)\n",
    "                df_csv = pd.read_csv(file_corrente)\n",
    "                if file_corrente.__contains__(\"anova.csv\"):\n",
    "                    anova = pd.concat([anova,df_csv],ignore_index=True)\n",
    "                elif file_corrente.__contains__(\"ExtraTree.csv\"):\n",
    "                    extra_tree = pd.concat([extra_tree,df_csv],ignore_index=True)\n",
    "                elif file_corrente.__contains__(\"RFE.csv\"):\n",
    "                    rfe = pd.concat([rfe,df_csv],ignore_index=True)\n",
    "                elif file_corrente.__contains__(\"SVM.csv\"):\n",
    "                    svm = pd.concat([svm,df_csv],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andiamo a normalizzare tutti gli score delle feature, in maniera tale da avere valori compresi tra 0 e 1\n",
    "\n",
    "anova['F_Score'] = anova['F_Score']/anova['F_Score'].max()\n",
    "extra_tree['F_Score'] = extra_tree['F_Score']/extra_tree['F_Score'].max()\n",
    "svm['F_Score'] = svm['F_Score']/svm['F_Score'].max()\n",
    "rfe['F_Score'] = rfe['F_Score']/rfe['F_Score'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facciamo una concatenazione dei vari dataframe.\n",
    "# Saranno utilizzati per prendere le feature che si ripetono di più e che hanno i punteggi maggiori\n",
    "\n",
    "total_feature = pd.concat([anova,extra_tree,svm,rfe],ignore_index=True)\n",
    "total_feature_count = pd.concat([anova,extra_tree,svm,rfe],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Input_Features  Conto\n",
      "47  Init Fwd Win Byts     23\n",
      "40   Fwd Seg Size Min     19\n",
      "46  Init Bwd Win Byts     17\n",
      "25        Flow Pkts/s     14\n",
      "34    Fwd Pkt Len Max     13\n",
      "..                ...    ...\n",
      "56       SYN Flag Cnt      1\n",
      "36    Fwd Pkt Len Min      1\n",
      "15     CWE Flag Count      1\n",
      "41      Fwd URG Flags      1\n",
      "33      Fwd PSH Flags      1\n",
      "\n",
      "[66 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Andiamo a eseguire un operazione di group by, con successivo count delle occorrenze dei vari campi\n",
    "\n",
    "total_feature_count = total_feature_count.groupby(['Input_Features']).count().reset_index().sort_values(by='F_Score',ascending=False)\n",
    "\n",
    "# Andiamo a rinominare la colonna F_Score, che adesso contiene i valori di count\n",
    "\n",
    "total_feature_count.rename(columns = {'F_Score':'Conto'}, inplace = True)\n",
    "print(total_feature_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Input_Features  Conto\n",
      "47  Init Fwd Win Byts     23\n",
      "40   Fwd Seg Size Min     19\n",
      "46  Init Bwd Win Byts     17\n",
      "25        Flow Pkts/s     14\n",
      "34    Fwd Pkt Len Max     13\n",
      "48       PSH Flag Cnt     12\n",
      "10    Bwd Pkt Len Max     12\n",
      "37    Fwd Pkt Len Std     12\n",
      "38         Fwd Pkts/s     12\n",
      "39   Fwd Seg Size Avg     11\n"
     ]
    }
   ],
   "source": [
    "# Delle delle feature più frequenti mi interessano le prime dieci\n",
    "\n",
    "most_frequent = total_feature_count[:10]\n",
    "\n",
    "print(most_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Input_Features   F_Score\n",
      "6    Init Fwd Win Byts  0.000005\n",
      "49   Init Fwd Win Byts  0.000755\n",
      "74   Init Fwd Win Byts  0.007213\n",
      "88   Init Fwd Win Byts  0.083067\n",
      "103  Init Fwd Win Byts  0.162237\n",
      "..                 ...       ...\n",
      "168   Fwd Seg Size Avg  0.120999\n",
      "181   Fwd Seg Size Avg  0.258833\n",
      "234   Fwd Seg Size Avg  0.292542\n",
      "329   Fwd Seg Size Avg  1.000000\n",
      "347   Fwd Seg Size Avg  1.000000\n",
      "\n",
      "[145 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ci serve ancora il parametro F_Scores, quindi creiamo un nuovo dataframe con queste righe.\n",
    "# Abbiamo le dieci righe, però, sono ancora presenti le ridondanze che prima avevamo eliminato.\n",
    "\n",
    "test = pd.DataFrame(data=[],columns=total_feature.columns)\n",
    "\n",
    "for a in most_frequent['Input_Features']:\n",
    "    test = pd.concat([test,total_feature[total_feature['Input_Features'] == a]])\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo una variabile all'interno del quale vado a salvare il groupby del precedente dataframe, solo che in uesto caso\n",
    "# andiamo a sommare i punteggi di F_Scores, invece di contare le occorrenze.\n",
    "\n",
    "group_by = test.groupby(['Input_Features'])['F_Score'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Input_Features    F_Score\n",
      "8  Init Fwd Win Byts  11.565773\n",
      "7  Init Bwd Win Byts   9.631053\n",
      "6   Fwd Seg Size Min   7.735106\n",
      "1        Flow Pkts/s   6.271195\n",
      "3    Fwd Pkt Len Std   4.895337\n",
      "0    Bwd Pkt Len Max   3.993803\n",
      "2    Fwd Pkt Len Max   3.638016\n",
      "4         Fwd Pkts/s   3.043988\n",
      "5   Fwd Seg Size Avg   2.861467\n",
      "9       PSH Flag Cnt   1.489924\n"
     ]
    }
   ],
   "source": [
    "# Facciamo un ordinamento del dataframe precedente\n",
    "\n",
    "print(group_by.sort_values(by='F_Score',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A questo punto, terminiamo l'operazione andando a moltiplicare la frequenza per la somma dei punteggi delle varie feature.\n",
    "\n",
    "last_feature = pd.DataFrame(data=[],columns=group_by.columns)\n",
    "\n",
    "for a,b in zip(most_frequent['Input_Features'],most_frequent['Conto']):\n",
    "    last_feature.loc[len(last_feature)] =  {'Input_Features':a,'F_Score':group_by[group_by['Input_Features'] == a]['F_Score'].values[0]*b}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Input_Features     F_Score\n",
      "0  Init Fwd Win Byts  266.012777\n",
      "2  Init Bwd Win Byts  163.727902\n",
      "1   Fwd Seg Size Min  146.967007\n",
      "3        Flow Pkts/s   87.796736\n",
      "7    Fwd Pkt Len Std   58.744040\n",
      "6    Bwd Pkt Len Max   47.925639\n",
      "4    Fwd Pkt Len Max   47.294212\n",
      "8         Fwd Pkts/s   36.527860\n",
      "9   Fwd Seg Size Avg   31.476138\n",
      "5       PSH Flag Cnt   17.879092\n"
     ]
    }
   ],
   "source": [
    "# Facciamo una stampa ordinata\n",
    "\n",
    "print(last_feature.sort_values(by='F_Score',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A questo punto ci interessano solo le prime 5 feature, e tutte le loro possibili combinazioni di due elementi.\n",
    "\n",
    "esito_finale = pd.DataFrame(data=[],columns=['Feature_1','Feature_2'])\n",
    "top_index = []\n",
    "\n",
    "# Qui andiamo a prendere gli indici delle prime 5 features\n",
    "\n",
    "for a in range(0,5):\n",
    "    top_index.append(last_feature.sort_values(by='F_Score',ascending=False).index[a])\n",
    "    \n",
    "# Andiamo a inserire tutte le possibili combinazioni nel dataframe\n",
    "\n",
    "for b in comb:\n",
    "\n",
    "    esito_finale.loc[len(esito_finale)] =  {\n",
    "        'Feature_1': last_feature.loc[top_index[b[0]]]['Input_Features'],\n",
    "        'Feature_2': last_feature.loc[top_index[b[1]]]['Input_Features']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Feature_1          Feature_2\n",
      "0  Init Fwd Win Byts  Init Bwd Win Byts\n",
      "1  Init Fwd Win Byts   Fwd Seg Size Min\n",
      "2  Init Fwd Win Byts        Flow Pkts/s\n",
      "3  Init Fwd Win Byts    Fwd Pkt Len Std\n",
      "4  Init Bwd Win Byts   Fwd Seg Size Min\n",
      "5  Init Bwd Win Byts        Flow Pkts/s\n",
      "6  Init Bwd Win Byts    Fwd Pkt Len Std\n",
      "7   Fwd Seg Size Min        Flow Pkts/s\n",
      "8   Fwd Seg Size Min    Fwd Pkt Len Std\n",
      "9        Flow Pkts/s    Fwd Pkt Len Std\n"
     ]
    }
   ],
   "source": [
    "print(esito_finale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Init Fwd Win Byts\n",
      "1    Init Bwd Win Byts\n",
      "2     Fwd Seg Size Min\n",
      "3          Flow Pkts/s\n",
      "4      Fwd Pkt Len Std\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "top5 = esito_finale.Feature_1.unique()\n",
    "top5 = np.append(top5,esito_finale.Feature_2.unique())\n",
    "top5 = pd.Series(top5)\n",
    "top5 = top5.unique()\n",
    "top5 = pd.Series(top5)\n",
    "print(top5)\n",
    "\n",
    "top5.to_csv('top_feature/top5_totale.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "esito_finale.to_csv('top_feature/combinazioni_totale.csv',index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
